<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记#1 梯度下降：一元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn <strong>without</strong> being explicitly programmed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to <em>learn</em> from <strong>experience</strong> E with respect to some <strong>task</strong> T and some <strong>performance measure</strong> P, if its performance on T, as measured by P, improves with experience E.</li></ul><h3 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习 | Supervised Learning"></a>监督学习 | Supervised Learning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，Support Vector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-Unsupervised-Learning"><a href="#无监督学习-Unsupervised-Learning" class="headerlink" title="无监督学习 | Unsupervised Learning"></a>无监督学习 | Unsupervised Learning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（Cluster Algorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-Univariate-Linear-Regression"><a href="#一元线性回归-Univariate-Linear-Regression" class="headerlink" title="一元线性回归 | Univariate Linear Regression"></a>一元线性回归 | Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：<br>$$<br>{(x(i),y(i)),i&#x3D;1,2,⋯,m}<br>$$</p><ul><li>$m$ 代表训练集中实例的数量；</li><li>$x $代表<strong>特征</strong> or 输入变量；</li><li>$y $代表<strong>目标变量</strong> or 输出变量；</li><li>$({x}^{(i)},{y}^{(i)})$ 代表第 i 个观察实例；</li><li>$h$ 代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设 $h$。在本文中，我们尝试用线性函数 $h_\theta \left( x \right)&#x3D;\theta_{0} + \theta_{1}x$ 拟合之。因为只含有一个特征 or 输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 | Cost Function"></a>代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）$\theta_{0}$ 和 $\theta_{1}$，即直线的斜率和在 $y$ 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modeling error）。我们要做的就是尽量选择参数使得误差降低，即最小化 $h_{\theta}(x^{(i)})$ 和 $h_{\theta}(y^{(i)})$ 的距离。于是我们有了经典的<strong>平方误差代价函数</strong>：<br>$$<br>J\left( \theta <em>0,\theta <em>1 \right) &#x3D;\frac{1}{2m}\sum</em>{i&#x3D;1}^m{\left( h</em>{\theta}(x^{(i)})-y^{(i)} \right) ^2}<br>$$<br>也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以 2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 $θ0$ 和 $θ1$ 和 $J(θ0,θ1)$，对每个 $(θ0,θ1)$ 对，代入训练集可以得到一个 $J(θ0,θ1) $值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><a href="https://hwcoder.top/img/blog/ML-Note-1-images/3d-surface.png"><img src="http://img.hexiaobo.xyz/3d-surface.png" alt="三维空间中的碗状曲面"></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 $J(\theta_{0}, \theta_{1}) $最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contour plot）：</p><p><a href="https://hwcoder.top/img/blog/ML-Note-1-images/contour.png"><img src="http://img.hexiaobo.xyz/contour.png" alt="二维空间的等高线图"></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 $J(\theta_{0}, \theta_{1})$ 最小的点。</p><h3 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法 | Gradient Descent"></a>梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数$ J $取最小值的 $$\left( \theta_0, \theta_1 \right)$$对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合$\left( \theta_{0},\theta_{1},……,\theta_{n} \right)$，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（local minimum），但我们无法确定其是否就是<strong>全局最小值</strong>（global minimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convex function），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J 的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：<br>$$<br>\nabla J&#x3D;\mathrm{grad} J&#x3D;\left{ \frac{\partial J}{\partial \theta <em>0},\frac{\partial J}{\partial \theta <em>1} \right}<br>$$<br>所以不断迭代进行赋值：<br>$$<br>\theta_j:&#x3D;\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j} J(\theta</em>{0}, \theta</em>{1})<br>$$<br>直到收敛，即可找到一个<strong>极小值</strong>。其中，α 就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnig rate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>对代价函数求偏导：</p><ul><li>$j&#x3D;0 $时：$\frac{\partial}{\partial \theta _0}J(\theta _0,\theta <em>1)&#x3D;\frac{1}{m}\sum</em>{i&#x3D;1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)} \right)}$</li><li>$j&#x3D;1$ 时：$\frac{\partial}{\partial \theta _1}J(\theta _0,\theta <em>1)&#x3D;\frac{1}{m}\sum</em>{i&#x3D;1}^m{x^{(i)}\left( \theta _1x^{(i)}+\theta _0-y^{(i)} \right)}$</li></ul><p>所以梯度方向为：<br>$$<br>\mathrm{grad } J&#x3D;\left{ \frac{\partial J}{\partial \theta _0},\frac{\partial J}{\partial \theta <em>1} \right} &#x3D;\left{ \frac{1}{m}\sum</em>{i&#x3D;1}^m{\left( \theta <em>1x^{(i)}+\theta <em>0-y^{(i)} \right)},\frac{1}{m}\sum</em>{i&#x3D;1}^mx^{(i)}\left( \theta <em>1x^{(i)}+\theta <em>0-y^{(i)} \right) \right}<br>$$<br>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如 $\sum</em>{i&#x3D;1}^mx^{(i)}$、$\sum</em>{i&#x3D;1}^my^{(i)}$、$\sum</em>{i&#x3D;1}^mx^{(i)}y^{(i)}$等项，这也称为<strong>批量梯度下降</strong>（Batch Gradient Descent）。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>需要注意的是，迭代赋值的过程中，(θ0,θ1) 的值要同时更新，否则就会与梯度方向有微小区别。用 Python 元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当$a$和 $b$都为向量时有 $a^Tb&#x3D;b^Ta$，所以有：<br>$$<br>X\theta &#x3D;\left[ \begin{array}{c}<br>    -\left( x^{(1)} \right) ^T\theta -\<br>    -\left( x^{(2)} \right) ^T\theta -\<br>    \vdots\<br>    -\left( x^{(m)} \right) ^T\theta -\<br>\end{array} \right] &#x3D;\left[ \begin{array}{c}<br>    -\theta ^T\left( x^{(1)} \right) -\<br>    -\theta ^T\left( x^{(2)} \right) -\<br>    \vdots\<br>    -\theta ^T\left( x^{(m)} \right) -\<br>\end{array} \right]<br>$$<br>下面以一元线性回归数据集 <code>ex1data1.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的$\left( \theta_0, \theta_1 \right)$ 结果是：[-3.8953 1.1929]，作图如下：</p><p><a href="https://hwcoder.top/img/blog/ML-Note-1-images/Figure_1.png"><img src="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归"></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab 中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用 <code>for</code> 循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
