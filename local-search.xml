<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>d</p><h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn <strong>without</strong> being explicitly programmed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to <em>learn</em> from <strong>experience</strong> E with respect to some <strong>task</strong> T and some <strong>performance measure</strong> P, if its performance on T, as measured by P, improves with experience E.</li></ul><h3 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习 | Supervised Learning"></a>监督学习 | Supervised Learning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，Support Vector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-Unsupervised-Learning"><a href="#无监督学习-Unsupervised-Learning" class="headerlink" title="无监督学习 | Unsupervised Learning"></a>无监督学习 | Unsupervised Learning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（Cluster Algorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-Univariate-Linear-Regression"><a href="#一元线性回归-Univariate-Linear-Regression" class="headerlink" title="一元线性回归 | Univariate Linear Regression"></a>一元线性回归 | Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：</p><p>{(x(i),y(i)),i&#x3D;1,2,⋯,m}</p><ul><li>m 代表训练集中实例的数量；</li><li>x 代表<strong>特征</strong> or 输入变量；</li><li>y 代表<strong>目标变量</strong> or 输出变量；</li><li>(x(i),y(i)) 代表第 i 个观察实例；</li><li>h 代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设 h。在本文中，我们尝试用线性函数 hθ(x)&#x3D;θ0+θ1x 拟合之。因为只含有一个特征 or 输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 | Cost Function"></a>代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）θ0 和 θ1，即直线的斜率和在 y 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modeling error）。我们要做的就是尽量选择参数使得误差降低，即最小化 hθ(x(i)) 和 y(i) 的距离。于是我们有了经典的<strong>平方误差代价函数</strong>：</p><p>J(θ0,θ1)&#x3D;12m∑i&#x3D;1m(hθ(x(i))−y(i))2</p><p>也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以 2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 θ0 和 θ1 和 J(θ0,θ1)，对每个 (θ0,θ1) 对，代入训练集可以得到一个 J(θ0,θ1) 值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><a href="https://hwcoder.top/img/blog/ML-Note-1-images/3d-surface.png"><img src="http://img.hexiaobo.xyz/3d-surface.png" alt="三维空间中的碗状曲面"></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 J(θ0,θ1) 最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contour plot）：</p><p><a href="https://hwcoder.top/img/blog/ML-Note-1-images/contour.png"><img src="http://img.hexiaobo.xyz/contour.png" alt="二维空间的等高线图"></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 J(θ0,θ1) 最小的点。</p><h3 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法 | Gradient Descent"></a>梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数 J 取最小值的 (θ0,θ1) 对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合(θ0,θ1,……,θn)，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（local minimum），但我们无法确定其是否就是<strong>全局最小值</strong>（global minimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convex function），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J 的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：</p><p>∇J&#x3D;gradJ&#x3D;{∂J∂θ0,∂J∂θ1}</p><p>所以不断迭代进行赋值：</p><p>θj:&#x3D;θj−α⋅∂∂θjJ(θ0,θ1)</p><p>直到收敛，即可找到一个<strong>极小值</strong>。其中，α 就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnig rate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>对代价函数求偏导：</p><ul><li>j&#x3D;0 时：∂∂θ0J(θ0,θ1)&#x3D;1m∑i&#x3D;1m(θ1x(i)+θ0−y(i))</li><li>j&#x3D;1 时：∂∂θ1J(θ0,θ1)&#x3D;1m∑i&#x3D;1mx(i)(θ1x(i)+θ0−y(i))</li></ul><p>所以梯度方向为：</p><p>gradJ&#x3D;{∂J∂θ0,∂J∂θ1}&#x3D;{1m∑i&#x3D;1m(θ1x(i)+θ0−y(i)),1m∑i&#x3D;1mx(i)(θ1x(i)+θ0−y(i))}</p><p>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如 ∑i&#x3D;1mx(i)、∑i&#x3D;1my(i)、∑i&#x3D;1mx(i)y(i) 等项，这也称为<strong>批量梯度下降</strong>（Batch Gradient Descent）。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>需要注意的是，迭代赋值的过程中，(θ0,θ1) 的值要同时更新，否则就会与梯度方向有微小区别。用 Python 元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当 a 和 b 都为向量时有 aTb&#x3D;bTa，所以有：</p><p>Xθ&#x3D;[−(x(1))Tθ−−(x(2))Tθ−⋮−(x(m))Tθ−]&#x3D;[−θT(x(1))−−θT(x(2))−⋮−θT(x(m))−]</p><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的一元线性回归数据集 <code>ex1data1.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的 (θ0,θ1) 结果是：[-3.8953 1.1929]，作图如下：</p><p><a href="https://hwcoder.top/img/blog/ML-Note-1-images/Figure_1.png"><img src="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归"></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab 中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用 <code>for</code> 循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2022保研回忆录</title>
    <link href="/2022%E4%BF%9D%E7%A0%94%E5%9B%9E%E5%BF%86%E5%BD%95.html"/>
    <url>/2022%E4%BF%9D%E7%A0%94%E5%9B%9E%E5%BF%86%E5%BD%95.html</url>
    
    <content type="html"><![CDATA[<h2 id="1、个人情况"><a href="#1、个人情况" class="headerlink" title="1、个人情况"></a>1、个人情况</h2><ul><li>本科：中部某四非 软件工程专业（非985、211、双一流）</li><li>排名：夏令营排名：2&#x2F;200+，预推免排名1&#x2F;200+</li><li>荣誉奖项：国家励志奖学金、省三好学生等</li><li>竞赛奖项：计算机设计大赛国二；其他一些算法编程类比赛、建模比赛国奖若干（感觉都没啥用）</li><li>科研经历：主持DL相关大创一项，无论文产出</li><li>最终去向：华南理工大学信通学硕（NLP方向）</li></ul><p><em>由于我没有六级，夏令营各种被拒，预推免也是十分艰难，错失了很多985，血与泪的教训，建议学弟学妹们一定要早点过六级！！！</em></p><h2 id="2、最初定位"><a href="#2、最初定位" class="headerlink" title="2、最初定位"></a>2、最初定位</h2><p>择校：由于本身是四非学校，目标初步确定<strong>中九</strong>，冲华五（我不想妥协，有个985梦，因此夏令营、预推免没有报除北邮以外的任何211）。</p><p>方向：CV或NLP，软件工程方向或其他AI方向都可。</p><p>填报学位：第一选择学硕，强组可考虑直博，或者可实习、地域好的专硕。</p><p>其他考虑：优先北上广深、宿舍条件、环境。</p><h2 id="3、夏令营营情况"><a href="#3、夏令营营情况" class="headerlink" title="3、夏令营营情况"></a>3、夏令营营情况</h2><p>入营：</p><table><thead><tr><th>学校</th><th>学院</th><th>备注</th></tr></thead><tbody><tr><td>电子科技大学</td><td>信息与软件工程学院、深圳高等研究院</td><td>成电夏令营纯宣讲，优营≠offer</td></tr><tr><td>中国农业大学</td><td>信息与电气与电气工程学院</td><td>优营≠offer。 没有方向合适的老师，拒绝面试</td></tr><tr><td>深圳大学</td><td>计算机与软件学院</td><td>优营。考虑地域因素报着玩。当时各种被拒，感谢深大给的第一个offer！</td></tr><tr><td>东南大学</td><td>网络与空间安全学院</td><td>候补300+。没想到最后也候补上了。。。</td></tr></tbody></table><p>其余的，北理工、北师大、哈工大、湖南大学、华东师大、南大、山大、中科大、华南理工都不出意外的寄了。。今年夏令营是真的好难啊，每个学校报名人数相比去年都翻倍了，还有我夏令营和老师发邮件联系都太晚了（当时在忙比赛），这也可能是导致我夏令营颗粒无收的一个重要原因。</p><p>到整个夏令营全部结束，我手里面也没有offer，说实话当时还是挺慌的。不过各大高校夏令营入营都是那一批佬，我坚信后面我肯定还是有机会的，趁着夏令营和预推免的之间一段空闲时间，我开始抓紧复习专业课、不断熟悉项目。</p><h2 id="4、夏令营细节"><a href="#4、夏令营细节" class="headerlink" title="4、夏令营细节"></a>4、夏令营细节</h2><h3 id="成电软院"><a href="#成电软院" class="headerlink" title="成电软院"></a>成电软院</h3><p>成电入营bar不高，应该是靠rank初筛，结合获奖，没卡六级。</p><p>成电夏令营纯宣讲，各组宣讲完成后会让营员报名一位老师，最后由报名老师对自己考核，选拔出所谓的“优秀营员”。当我报的老师，只是简单了自我介绍、讲了自己的比赛和项目，没有问任何项目细节，感觉就是纯聊天。</p><h3 id="农大信电"><a href="#农大信电" class="headerlink" title="农大信电"></a>农大信电</h3><p>农大不刷人，报了就入营</p><p>同上也是纯宣讲，宣讲完之后选择的导师会通知面试，由于当时对这些老师的方向都不感兴趣，遂拒绝了面试。</p><p>ps：不过农大的食堂是真的好，吃货可以考虑哈哈</p><h3 id="成电深高院"><a href="#成电深高院" class="headerlink" title="成电深高院"></a>成电深高院</h3><p>入营bar同本部。</p><p>和本部流程基本一致，夏令营不发offer。</p><h3 id="东南网安"><a href="#东南网安" class="headerlink" title="东南网安"></a>东南网安</h3><p>东南夏令营和预推免合并了，是夏令营最晚的也是预推免最早的。东南入营基本也是靠rank筛，四非rank 1、2加一些奖项都是很稳的，夏令营入营400+，超大营。</p><p>东南网安今年先让选择去南京还是无锡，去年是统一面试再分配的。</p><p>南京和无锡分开面试，流程大致都一样，20分钟一个人。面试不能使用PPT，首先是3分钟自我介绍，然后是英语提问，最后会根据提交的报名表上的获奖和科研经历进行提问，专业课有的组会问，不过主要还是问项目的多。最后面试排名南京300+，寄。。。</p><h2 id="5、预推免情况"><a href="#5、预推免情况" class="headerlink" title="5、预推免情况"></a>5、预推免情况</h2><p>预推免入营：</p><ul><li>华南理工大学 未来技术学院</li><li>电子科技大学 深圳高等研究院</li><li>哈尔滨工业大学 卓越工程师学院</li></ul><p><em>结果：电子科技大学 offer、华南理工大学候补（8左右）</em></p><p>预推免前期阶段，我开始疯狂发邮件，不过回复我的不足1&#x2F;10，回复了大部分也是没名额了，去年学长的预推免可不是这样的啊555。</p><p>在9月初，终于有一位华南理工的老师给我回复，给我打电话。经过交流觉得这位老师人很不错，方向也比较喜欢，经过面试后，老师表示愿意给我留一个位置，当时非常激动，总算遇到了一位比较满意并愿意接受我的老师。</p><h2 id="6、预推免细节"><a href="#6、预推免细节" class="headerlink" title="6、预推免细节"></a><strong>6、预推免细节</strong></h2><p>预推免阶段，会面临好多学校同时报名，有点应付不过来，最终报名的这三所学校也都顺利入营。</p><h4 id="哈工大卓越工程师"><a href="#哈工大卓越工程师" class="headerlink" title="哈工大卓越工程师"></a><strong>哈工大卓越工程师</strong></h4><p>第二年招生的学院，去年基本没人报，今年人数剧增。</p><p>面试一共7分钟，感觉纯背景面。先是英语3分钟自我介绍，随后进行PPT展示个人情况、科研经历等，我从开始讲到结束，老师都没说一句话。。。当时懵逼了，讲完就让直接退出，QAQ，可能我是当天最后几个面试的，都不想问了。</p><p>面试体验很不友好，全程对着黑屏讲，而且最离谱的是面试前还要在腾讯会议开着视频等着轮到自己，纯纯罚坐一下午，表示不能理解。</p><p>结果是无了，而且哈工大的骚操作居然没有候补，不知道招办哪来的底气。</p><h4 id="电子科大深圳"><a href="#电子科大深圳" class="headerlink" title="电子科大深圳"></a><strong>电子科大深圳</strong></h4><p>电科深圳和本部面试流程基本类似，都是20分钟时间。首先是3分钟自我介绍，5分钟英语问答，剩下时间专家提问，主要是根据自我介绍以及提交的简历，由于我项目介绍的比较多，就没问专业课。整个面试流程 下来感觉还是挺舒服的，老师都挺友好。</p><p>由于前面有人放弃offer，我以最后一名拿到了电深的offer（报名方向一共招5人），也算是预推免的第一个正式offer。</p><h4 id="华工未来技术学院"><a href="#华工未来技术学院" class="headerlink" title="华工未来技术学院"></a><strong>华工未来技术学院</strong></h4><p>华南理工未来学院的面试和电信学院面试都是一样的（去年未来学院是和电信学院一起面试）。面试一共15分钟，直接进行PPT展示，前2分钟是英语汇报，后面3分钟是中文汇报（本想全英汇报，结果被无情打断，要求后面用中文）。</p><p>英语提问环节问了我一个不是很难的问题，由于紧张没回答好。后面项目提问环节基本都回答的不错，但是由于英语部分过于拉跨，面试没有达到录取线，学硕候补8名左右。</p><p>不过感到暖心的是导师答应会一直给我保留名额，不会接收其他学生（华工优营需要和导师完成双选），结果在9月28当天很幸运的候补到我了，感谢老师！</p><h2 id="7、最终去向"><a href="#7、最终去向" class="headerlink" title="7、最终去向"></a><strong>7、最终去向</strong></h2><p>到9月28这天，手里有<strong>华南理工未来技术学院</strong>（学硕）、<strong>电子科大深圳</strong>（专硕）、<strong>东南网安</strong>（专硕）、<strong>深圳大学计软</strong>（学硕）这些offer。</p><p>最终在9月28中午，我毫无疑问的接受了华南理工的offer，中九、学硕、广州，符合我的一切需求了。至此我的保研完整的结束，虽然过程一波三折，但最后得到了一个满意的结果我也无憾了。</p><h2 id="8、总结"><a href="#8、总结" class="headerlink" title="8、总结"></a><strong>8、总结</strong></h2><ol><li><strong>一定要提前联系导师，越早越好</strong>。尽早做好简历，至少让老师能够通过简历认可你就足够。</li><li><strong>和老师真诚相待，保持沟通。</strong>如果老师认可了你，你也表现出强烈想去的意愿，只要导师人品没什么问题，大多数导师是愿意给你保留位置的，对入营面试都会有帮助的。</li><li><strong>复习专业课一定要提前。</strong>当夏令营开始时候几乎没有时间去准备专业课，报名会占用大量时间。因此对于自己想要去的学校提前准备相应的专业课。</li><li><strong>任何时候都别放弃，四非也有学上</strong>。今年夏令营、预推免面试全部都是线上，学生面试的成本大大降低，9月28当天，鸽子漫天飞，大量学校被鸽穿。同专业同学，在9月30成功捡漏上岸电子科大软院专硕，还是一位不错的导师，因为优营的大佬都会有更好的去向，所以不必担心没学上。</li></ol><h2 id="9、后记"><a href="#9、后记" class="headerlink" title="9、后记"></a>9、后记</h2><p>回想保研的经历感慨颇多，最初我是希望去北邮的，毕竟往届的学长学姐们上岸中九的少之又少，北邮已是最好的选择了。但我从夏令营到预推免都没能联系上北邮的导师，一直到9月28当天北邮才有一位之前联系过感觉比较满意的导师给我回复（可能是被鸽了才来找我hh）。</p><p>我觉得每个人的保研经历都是不可复制的，每个人的机遇都不相同，实力之外，被导师认同是非常重要的，本科背景不好不代表没有机会，去海投，总有能够认可自己的导师。</p><p>最后，感谢我的父母的支持和各位本科老师、外校老师、学长学姐、各位保研er的帮助，这个过程中认识了许多其他学校优秀的小伙伴，和他们交流收获了许多。</p>]]></content>
    
    
    <categories>
      
      <category>心情随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>python环境配置</title>
    <link href="/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.html"/>
    <url>/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.html</url>
    
    <content type="html"><![CDATA[<h2 id="1-anaconda-下载安装"><a href="#1-anaconda-下载安装" class="headerlink" title="1.anaconda  下载安装"></a>1.anaconda  下载安装</h2><p>正常下载安装，环境变量path配置 conda目录和scripts路径</p><h2 id="2-设置国内镜像"><a href="#2-设置国内镜像" class="headerlink" title="2.设置国内镜像"></a>2.设置国内镜像</h2><p>如果需要安装很多packages，你会发现conda下载的速度经常很慢，因为Anaconda.org的服务器在国外。所幸的是，清华TUNA镜像源有Anaconda仓库的镜像，我们将其加入conda的配置即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 添加Anaconda的TUNA镜像</span><br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br><span class="hljs-comment"># TUNA的help中镜像地址加有引号，需要去掉</span><br> <br><span class="hljs-comment"># 设置搜索时显示通道地址</span><br>conda config --<span class="hljs-built_in">set</span> show_channel_urls yes<br></code></pre></td></tr></table></figure><h2 id="3-安装opencv时候源有问题"><a href="#3-安装opencv时候源有问题" class="headerlink" title="3.安装opencv时候源有问题"></a>3.安装opencv时候源有问题</h2><p>opencv下载：<a href="https://pypi.tuna.tsinghua.edu.cn/simple/opencv-contrib-python/">Links for opencv-contrib-python (tsinghua.edu.cn)</a></p><p>找对应py版本</p><p>user下的condarc文件内容改成如下（加上了win-64）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">ssl_verify: true<br>show_channel_urls: true<br><br>channels:<br><br>  - http:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/anaconda/</span>pkgs<span class="hljs-regexp">/free/</span>win-<span class="hljs-number">64</span>/<br>  - http:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/anaconda/</span>pkgs<span class="hljs-regexp">/main/</span>win-<span class="hljs-number">64</span>/<br></code></pre></td></tr></table></figure><h2 id="4-安装dilib库"><a href="#4-安装dilib库" class="headerlink" title="4.安装dilib库"></a>4.安装dilib库</h2><p><img src="http://img.hexiaobo.xyz/image-20220309230544657.png" alt="安装dlib"></p>]]></content>
    
    
    <categories>
      
      <category>技术经验</category>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/hello-world.html"/>
    <url>/hello-world.html</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
