<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记#2 梯度下降：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>在前文 <ahref="http://hexiaobo.xyz/ML学习笔记-1-梯度下降：一元线性回归">一元线性回归</a>的基础上，我们引入多个特征变量，探讨梯度下降对多元线性回归的解法。此外，下一节将介绍正规方程在解多元线性回归中的应用。</p><h2 id="多元线性回归-multiple-linear-regression">多元线性回归 | MultipleLinear Regression</h2><p>现在我们的样本点 <span class="math inline">\(\left(x^{(i)},y^{(i)}\right)\)</span>有多个特征作为<strong>输入变量</strong>，即给定的数据集为： <spanclass="math display">\[\left\{\left(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span></p><ul><li><span class="math inline">\(n\)</span> 代表单个样本的特征数量；</li><li><span class="math inline">\({x}^{(i)}\)</span> 代表第 i个观察实例的<strong>特征向量</strong>；</li><li><span class="math inline">\(x^{(i)}_j\)</span> 代表第 i个观察实例的第$ j $个<strong>特征分量</strong>。</li></ul><p>同时，回归方程 h 也具有多个参数<spanclass="math inline">\(\theta_0,\theta_1,\cdots,\theta_n\)</span>： <spanclass="math display">\[h_\theta(x)=\theta_0+\theta_1x_1\cdots+\theta_nx_n\]</span> 为简化表达式，这里假定<span class="math inline">\(x_0 \equiv1\)</span> ，并以<strong>向量</strong>（vector）表示参数和自变量：<spanclass="math inline">\(\theta=(\theta_0,\cdots,\theta_n)^T,\;x=(x_0,\cdots,x_n)^T\)</span>，得到：<span class="math display">\[h_\theta(x)=\theta^Tx\]</span></p><h3 id="多变量梯度下降">多变量梯度下降</h3><p>类似地，我们定义平方误差代价函数： <span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span>我们的目标和一元线性回归中一样，要找出使得代价函数最小的一系列参数。于是，<span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 梯度下降时，不断作迭代： <span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}\]</span> 即可。</p><h3 id="特征缩放与标准化-standardization">特征缩放与标准化 |Standardization</h3><p>当不同自变量取值范围相差较大时，绘制的<strong>等高线图</strong>上的椭圆会变得瘦长，而梯度下降<strong>算法收敛</strong>将会很慢，因为每一步都可能会跨过这个椭圆导致<strong>振荡</strong>。这里略去数学上的证明。同理，所有依赖于「<strong>距离计算</strong>」的机器学习算法也会有此问题。</p><p>此时，我们需要把所有<strong>自变量</strong>（除了假定的x0）进行缩放、标准化，使其落在 -1 到 1 之间。最简单的方法是，置： <spanclass="math display">\[x_i^{(j)}:=\frac{x_i^{(j)}-\mu_i}{\sigma_i}\]</span> 其中，<spanclass="math inline">\(\mu_i=\frac{1}{m}\sum\limits_{j=1}^mx_i^{(j)}\)</span>是样本<strong>均值</strong>（Mean Value），<spanclass="math inline">\(\sigma_i=\sqrt{\frac{\sum\limits_{j=1}^m\left(x_i^{(j)}-\mu_i\right)^2}{m-1}}\)</span>是样本<strong>无偏标准差</strong>（UnbiasedStanderdDeviation），就完成了<strong>标准化</strong>（Standardization）。标准化后样本均值为0，方差为1，但不一定是标准正态分布（与其原始分布有关），根据中心极限定理可以推出。</p><p>需要注意的是，<strong>因变量</strong>不需要标准化，否则计算的结果将失真。且如果进行了标准化，对所有<strong>待测</strong>样本点也需要进行一样的操作，参数才能生效。</p><blockquote><p>此外，线性回归并不适用于所有情形，有时我们需要曲线来适应我们的数据，这时候我们也要对特征进行<strong>构造</strong>，如二次函数、三次函数、幂函数、对数函数等。构造后的新变量就可以当作一个新的特征来使用，这就是<strong>多项式回归</strong>（PolynomialRegression）。新变量的取值范围可能更大，此时，特征缩放就非常有必要！</p></blockquote><h3 id="归一化-normalization">归一化 | Normalization</h3><p>人们经常会混淆标准化（Standardization）与<strong>归一化</strong>（Normalization）的概念，这里也简单提一下：归一化的目的是找到某种映射关系，将原数据<strong>固定映射</strong>到某个区间<span class="math inline">\([a,b]\)</span>上，而标准化则没有限制。</p><p>归一化最常用于把有量纲数转化为<strong>无量纲数</strong>，让不同维度之间的特征在数值上有一定比较性，比如Min-Max Normalization： <span class="math display">\[x_{i}^{(j)}:=\frac{x_{i}^{(j)}-\min \left( x_i \right)}{\max \left( x_i\right) -\min \left( x_i \right)}\]</span>但是，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。因为「<strong>仅由极值决定</strong>」这个做法过于危险，如果样本中有一个异常大的值，则会将所有正常值挤占到很小的区间，而标准化方法则更加「弹性」，会兼顾所有样本。</p><h3 id="学习率-α">学习率 α</h3><p>上一节谈到，学习率（Learnigrate）的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。通过绘制<strong>迭代收敛曲线</strong>（ConvergenceGraph）可以看出学习率的好坏，也可以看出何时算法能收敛并及时<strong>终止算法</strong>。</p><p><a href="http://img.hexiaobo.xyz/cost-iter.png"><imgsrc="http://img.hexiaobo.xyz/cost-iter.png"alt="代价函数-迭代次数" /></a></p><p>代价函数-迭代次数</p><p>特别地，当 <span class="math inline">\(\alpha\)</span>取值过大时，曲线可能呈现<strong>上扬</strong>或<strong>波浪线</strong>型，解决办法都是选择更小的α 值。可以证明，只要 <spanclass="math inline">\(\alpha\)</span>足够小，凸函数都会收敛于极点。</p><p>此外，还有一种终止算法的方法：判断在某次或连续$ n $次迭代后 <spanclass="math inline">\(J(θ)\)</span>的变化小于某个极小量，如<spanclass="math inline">\(\varepsilon=1e^{-3}\)</span>，此时就可以认为算法终止。但这种办法则不能用于选择尽量大的<span class="math inline">\(α\)</span> 值。</p><h3 id="代码实现">代码实现</h3><p>下面以 <code>ex1data2.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># normalization</span><br>mu = X.mean(axis=<span class="hljs-number">0</span>)<br>sigma = X.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>X = (X - mu) / sigma<br>X = np.c_[np.ones(m), X] <span class="hljs-comment"># 增加一列 1</span><br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">1500</span><br>theta = np.zeros(n)<br>J_history = np.zeros(num_iters)<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (47, )</span><br>theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>)<br>J_history[i] = np.<span class="hljs-built_in">sum</span>(np.power(error, <span class="hljs-number">2</span>)) / (<span class="hljs-number">2</span> * m)<br><br><span class="hljs-comment"># predict</span><br>predict = (np.array([<span class="hljs-number">1650</span>, <span class="hljs-number">3</span>]) - mu) / sigma<br>predict = np.r_[<span class="hljs-number">1</span>, predict]<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br><span class="hljs-comment"># plot the convergence graph</span><br>plt.plot(np.arange(J_history.size), J_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of iterations&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost J&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1, \theta_2\right)\)</span>结果是：[340412.6595 110631.0484-6649.4724]，预测在<span class="math inline">\(\left( x_1=1650,x_2=3\right)\)</span>时的房价为 293101.0568574823。</p><p>绘制的迭代收敛曲线如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png"alt="多元线性回归的迭代收敛曲线" /></a></p><p>多元线性回归的迭代收敛曲线</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#1 梯度下降：一元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<h2 id="概论">概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that givescomputers the ability to learn <strong>without</strong> being explicitlyprogrammed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer programis said to <em>learn</em> from <strong>experience</strong> E withrespect to some <strong>task</strong> T and some <strong>performancemeasure</strong> P, if its performance on T, as measured by P, improveswith experience E.</li></ul><h3 id="监督学习-supervised-learning">监督学习 | SupervisedLearning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，SupportVector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-unsupervised-learning">无监督学习 | UnsupervisedLearning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（ClusterAlgorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-univariate-linear-regression">一元线性回归 |Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：<span class="math display">\[{(x(i),y(i)),i=1,2,⋯,m}\]</span></p><ul><li><span class="math inline">\(m\)</span> 代表训练集中实例的数量；</li><li>$x $代表<strong>特征</strong> or 输入变量；</li><li>$y $代表<strong>目标变量</strong> or 输出变量；</li><li><span class="math inline">\(({x}^{(i)},{y}^{(i)})\)</span> 代表第 i个观察实例；</li><li><span class="math inline">\(h\)</span>代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设<span class="math inline">\(h\)</span>。在本文中，我们尝试用线性函数<span class="math inline">\(h_\theta \left( x \right)=\theta_{0} +\theta_{1}x\)</span> 拟合之。因为只含有一个特征 or输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-cost-function">代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）<spanclass="math inline">\(\theta_{0}\)</span> 和 <spanclass="math inline">\(\theta_{1}\)</span>，即直线的斜率和在 <spanclass="math inline">\(y\)</span> 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modelingerror）。我们要做的就是尽量选择参数使得误差降低，即最小化 <spanclass="math inline">\(h_{\theta}(x^{(i)})\)</span> 和 <spanclass="math inline">\(h_{\theta}(y^{(i)})\)</span>的距离。于是我们有了经典的<strong>平方误差代价函数</strong>： <spanclass="math display">\[J\left( \theta _0,\theta _1 \right) =\frac{1}{2m}\sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}\]</span> 也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 <span class="math inline">\(\theta_0\)</span> 和 <span class="math inline">\(\theta _1\)</span> 和 <spanclass="math inline">\(J(\theta _0,\theta _1)\)</span>，对每个 <spanclass="math inline">\((\theta _0,\theta _1)\)</span>对，代入训练集可以得到一个 <span class="math inline">\(J(\theta_0,\theta_1)\)</span>值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><a href="http://img.hexiaobo.xyz/3d-surface.png"><imgsrc="http://img.hexiaobo.xyz/3d-surface.png"alt="三维空间中的碗状曲面" /></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 <spanclass="math inline">\(J(\theta_{0},\theta_{1})\)</span>最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contourplot）：</p><p><a href="http://img.hexiaobo.xyz/contour.png"><imgsrc="http://img.hexiaobo.xyz/contour.png"alt="二维空间的等高线图" /></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span> 最小的点。</p><h3 id="梯度下降法-gradient-descent">梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数$J $取最小值的 <span class="math display">\[\left( \theta_0, \theta_1\right)\]</span>对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合<spanclass="math inline">\(\left( \theta_{0},\theta_{1},……,\theta_{n}\right)\)</span>，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（localminimum），但我们无法确定其是否就是<strong>全局最小值</strong>（globalminimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convexfunction），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：<span class="math display">\[\nabla J=\mathrm{grad} J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\}\]</span> 所以不断迭代进行赋值： <span class="math display">\[\theta_j:=\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j}J(\theta_{0}, \theta_{1})\]</span> 直到收敛，即可找到一个<strong>极小值</strong>。其中，α就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnigrate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导">数学推导</h3><p>对代价函数求偏导：</p><ul><li><span class="math inline">\(j=0\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _0}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right)}\)</span></li><li><span class="math inline">\(j=1\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _1}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{x^{(i)}\left( \theta_1x^{(i)}+\theta _0-y^{(i)} \right)}\)</span></li></ul><p>所以梯度方向为： <span class="math display">\[\mathrm{grad } J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\} =\left\{\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)}\right)},\frac{1}{m}\sum_{i=1}^mx^{(i)}\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right) \right\}\]</span>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如<span class="math inline">\(\sum_{i=1}^mx^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^my^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^mx^{(i)}y^{(i)}\)</span>等项，这也称为<strong>批量梯度下降</strong>（BatchGradient Descent）。</p><h3 id="代码实现">代码实现</h3><p>需要注意的是，迭代赋值的过程中，<span class="math inline">\(\left(\theta_0, \theta_1\right)\)</span>的值要同时更新，否则就会与梯度方向有微小区别。用 Python元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当<spanclass="math inline">\(a\)</span>和 <spanclass="math inline">\(b\)</span>都为向量时有 <spanclass="math inline">\(a^Tb=b^Ta\)</span>，所以有： <spanclass="math display">\[X\theta =\left[ \begin{array}{c}    -\left( x^{(1)} \right) ^T\theta -\\    -\left( x^{(2)} \right) ^T\theta -\\    \vdots\\    -\left( x^{(m)} \right) ^T\theta -\\\end{array} \right] =\left[ \begin{array}{c}    -\theta ^T\left( x^{(1)} \right) -\\    -\theta ^T\left( x^{(2)} \right) -\\    \vdots\\    -\theta ^T\left( x^{(m)} \right) -\\\end{array} \right]\]</span> 下面以一元线性回归数据集 <code>ex1data1.txt</code>为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1\right)\)</span> 结果是：[-3.8953 1.1929]，作图如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归" /></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用<code>for</code>循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
