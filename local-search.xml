<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记#1 梯度下降：一元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<h2 id="概论">概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that givescomputers the ability to learn <strong>without</strong> being explicitlyprogrammed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer programis said to <em>learn</em> from <strong>experience</strong> E withrespect to some <strong>task</strong> T and some <strong>performancemeasure</strong> P, if its performance on T, as measured by P, improveswith experience E.</li></ul><h3 id="监督学习-supervised-learning">监督学习 | SupervisedLearning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，SupportVector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-unsupervised-learning">无监督学习 | UnsupervisedLearning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（ClusterAlgorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-univariate-linear-regression">一元线性回归 |Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：<span class="math display">\[{(x(i),y(i)),i=1,2,⋯,m}\]</span></p><ul><li><span class="math inline">\(m\)</span> 代表训练集中实例的数量；</li><li>$x $代表<strong>特征</strong> or 输入变量；</li><li>$y $代表<strong>目标变量</strong> or 输出变量；</li><li><span class="math inline">\(({x}^{(i)},{y}^{(i)})\)</span> 代表第 i个观察实例；</li><li><span class="math inline">\(h\)</span>代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设<span class="math inline">\(h\)</span>。在本文中，我们尝试用线性函数<span class="math inline">\(h_\theta \left( x \right)=\theta_{0} +\theta_{1}x\)</span> 拟合之。因为只含有一个特征 or输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-cost-function">代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）<spanclass="math inline">\(\theta_{0}\)</span> 和 <spanclass="math inline">\(\theta_{1}\)</span>，即直线的斜率和在 <spanclass="math inline">\(y\)</span> 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modelingerror）。我们要做的就是尽量选择参数使得误差降低，即最小化 <spanclass="math inline">\(h_{\theta}(x^{(i)})\)</span> 和 <spanclass="math inline">\(h_{\theta}(y^{(i)})\)</span>的距离。于是我们有了经典的<strong>平方误差代价函数</strong>： <spanclass="math display">\[J\left( \theta _0,\theta _1 \right) =\frac{1}{2m}\sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}\]</span> 也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 <spanclass="math inline">\(θ0\)</span> 和 <spanclass="math inline">\(θ1\)</span> 和 <spanclass="math inline">\(J(θ0,θ1)\)</span>，对每个 <spanclass="math inline">\((θ0,θ1)\)</span> 对，代入训练集可以得到一个$J(θ0,θ1)$值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><ahref="https://hwcoder.top/img/blog/ML-Note-1-images/3d-surface.png"><imgsrc="http://img.hexiaobo.xyz/3d-surface.png"alt="三维空间中的碗状曲面" /></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 $J(<em>{0}, </em>{1})$最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contourplot）：</p><p><ahref="https://hwcoder.top/img/blog/ML-Note-1-images/contour.png"><imgsrc="http://img.hexiaobo.xyz/contour.png"alt="二维空间的等高线图" /></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span> 最小的点。</p><h3 id="梯度下降法-gradient-descent">梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数$J $取最小值的 <span class="math display">\[\left( \theta_0, \theta_1\right)\]</span>对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合<spanclass="math inline">\(\left( \theta_{0},\theta_{1},……,\theta_{n}\right)\)</span>，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（localminimum），但我们无法确定其是否就是<strong>全局最小值</strong>（globalminimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convexfunction），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：<span class="math display">\[\nabla J=\mathrm{grad} J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\}\]</span> 所以不断迭代进行赋值： <span class="math display">\[\theta_j:=\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j}J(\theta_{0}, \theta_{1})\]</span> 直到收敛，即可找到一个<strong>极小值</strong>。其中，α就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnigrate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导">数学推导</h3><p>对代价函数求偏导：</p><ul><li>$j=0 <spanclass="math inline">\(时：\)</span>J(_0,<em>1)=</em>{i=1}^m{(_1x^{(i)}+_0-y^{(i)} )}$</li><li><span class="math inline">\(j=1\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _1}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{x^{(i)}\left( \theta_1x^{(i)}+\theta _0-y^{(i)} \right)}\)</span></li></ul><p>所以梯度方向为： <span class="math display">\[\mathrm{grad } J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\} =\left\{\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)}\right)},\frac{1}{m}\sum_{i=1}^mx^{(i)}\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right) \right\}\]</span>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如<span class="math inline">\(\sum_{i=1}^mx^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^my^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^mx^{(i)}y^{(i)}\)</span>等项，这也称为<strong>批量梯度下降</strong>（BatchGradient Descent）。</p><h3 id="代码实现">代码实现</h3><p>需要注意的是，迭代赋值的过程中，(θ0,θ1)的值要同时更新，否则就会与梯度方向有微小区别。用 Python元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当<spanclass="math inline">\(a\)</span>和 <spanclass="math inline">\(b\)</span>都为向量时有 <spanclass="math inline">\(a^Tb=b^Ta\)</span>，所以有： <spanclass="math display">\[X\theta =\left[ \begin{array}{c}    -\left( x^{(1)} \right) ^T\theta -\\    -\left( x^{(2)} \right) ^T\theta -\\    \vdots\\    -\left( x^{(m)} \right) ^T\theta -\\\end{array} \right] =\left[ \begin{array}{c}    -\theta ^T\left( x^{(1)} \right) -\\    -\theta ^T\left( x^{(2)} \right) -\\    \vdots\\    -\theta ^T\left( x^{(m)} \right) -\\\end{array} \right]\]</span> 下面以一元线性回归数据集 <code>ex1data1.txt</code>为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1\right)\)</span> 结果是：[-3.8953 1.1929]，作图如下：</p><p><ahref="https://hwcoder.top/img/blog/ML-Note-1-images/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归" /></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用<code>for</code>循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
