<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记#4 逻辑回归：二分类到多分类</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB.html</url>
    
    <content type="html"><![CDATA[<p>前文介绍了基本的线性回归问题，尝试预测一系列<strong>连续值</strong>属性。现在我们将介绍的分类问题则关注<strong>离散值</strong>属性的预测。</p><p>在分类问题中，我们尝试预测的结果是否属于某一个类，最基础的就是<strong>二元</strong>的分类问题（例如预测垃圾邮件、恶性肿瘤），更为复杂的则是预测<strong>多元</strong>的分类问题。</p><h2 id="二分类问题">二分类问题</h2><p>分类问题的样本与回归问题类似，由特征和目标构成，给定数据集：</p><p><spanclass="math display">\[\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span></p><ul><li><span class="math inline">\(x(i)\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个观察实例的 <spanclass="math inline">\(n+1\)</span> 维<strong>特征向量</strong> <spanclass="math inline">\(\left(x_0^{(i)},\cdots,x_n^{(i)}\right)^T\)</span>；</li><li><span class="math inline">\(y^{(i)}\in\{0,1\}\)</span> 代表第 <spanclass="math inline">\(i\)</span>个观察实例的<strong>目标变量</strong>，在这里有 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span> 两类结果。</li></ul><p>即对于输入的自变量 <span class="math inline">\(x^(i)\)</span>，因变量<span class="math inline">\(y^(i)\)</span> 可能为 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span>。其中 <spanclass="math inline">\(0\)</span> 表示<strong>负向类</strong>（negativeclass），<span class="math inline">\(1\)</span>表示<strong>正向类</strong>（positive class）。</p><blockquote><p>我们不对「正向」和「负向」加以特殊区分，但在实际应用中「正向」通常表示「具有我们要寻找的东西」，如垃圾邮件、恶性肿瘤等。</p></blockquote><h3 id="线性回归的不足">线性回归的不足</h3><p>首先可能会自然而然地想到用之前的线性回归来解决——用一条直线拟合结果，当预测值大于0.5 时归为正向类，反之归为负向类。</p><p>这看似合理，然而，线性回归保留了 <spanclass="math inline">\(y^(i)\)</span>太多的「<strong>信息量</strong>」。对于某些「<strong>反常样本</strong>」，我们可能预测出一个远大于<span class="math inline">\(1\)</span> 或者远小于 <spanclass="math inline">\(0\)</span>的结果，同理，这些「反常样本」用于拟合直线时也会对其造成一定偏移，以至于正常样本被归为错误类别。</p><figure><img src="http://img.hexiaobo.xyz/linear-regression.png"alt="反常样本使得蓝线偏移" /><figcaption aria-hidden="true">反常样本使得蓝线偏移</figcaption></figure><h2 id="逻辑回归-logistic-regression">逻辑回归 | LogisticRegression</h2><p>线性回归和逻辑回归都属于<strong>广义线性模型</strong>（GeneralizedLinearModel）的特殊形式，线性模型都可用于回归问题的求解。但由于<strong>逻辑函数</strong>（LosisticFunction）将结果映射到 <strong>Bernoulli分布</strong>，因此逻辑回归更常用于分类问题。</p><h3 id="假说表示-hypothesis-representation">假说表示 | HypothesisRepresentation</h3><p>回忆线性回归的假设函数：<spanclass="math inline">\(h_\theta(x)=\theta^Tx\)</span>，我们在其外套上<span class="math inline">\(sigmoid\)</span>函数，构造逻辑回归的假设函数为：</p><p><spanclass="math display">\[h_\theta(x)=g\left(\theta^Tx\right)=\frac{1}{1+e^{-\theta^Tx}}\]</span></p><blockquote><p>所谓 sigmoid 函数（也即前面提到的<strong>逻辑函数</strong>）：</p><p><span class="math display">\[g(z)=\frac{1}{1+e^{-z}}\]</span></p><figure><img src="http://img.hexiaobo.xyz/sigmoid.png" alt="sigmoid函数" /><figcaption aria-hidden="true">sigmoid函数</figcaption></figure><p>是一个介于 <span class="math inline">\((0,1)\)</span>之间的单增 <spanclass="math inline">\(S\)</span> 形函数，其导出需要用到 GLMs和指数分布族（The Exponential Family）的知识。</p></blockquote><p>也就是说，对于一个参数为 <span class="math inline">\(θ\)</span>的逻辑回归模型，输入 <span class="math inline">\(x\)</span>，得到 <spanclass="math inline">\(h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\)</span>的预测值。</p><p>我们可以把这个输出值视为 <span class="math inline">\(x\)</span>这个样本对应的 <span class="math inline">\(y\)</span> 等于 1的<strong>概率</strong>（estimated probablity），即 <spanclass="math inline">\(h_\theta \left( x \right)=P\left( y=1|x;\theta\right)\)</span>。针对分类情形，我们可以认为如果概率 <spanclass="math inline">\(\geqslant 0.5\)</span>，则分类为 <spanclass="math inline">\(1\)</span>，否则分类为 <spanclass="math inline">\(0\)</span>。</p><h3 id="决策边界-decision-boundary">决策边界 | Decision Boundary</h3><p>又根据 <span class="math inline">\(sigmoid\)</span> 函数的性质:</p><p><span class="math display">\[h_\theta(x)\geqslant 0.5\iff\theta^Tx\geqslant0\]</span></p><p>所以只要 <spanclass="math inline">\(\theta^Tx\geqslant0\)</span>，就会分类为 <spanclass="math inline">\(1\)</span>，否则分类为 <spanclass="math inline">\(0\)</span>；于是乎，<spanclass="math inline">\(\theta^Tx=0\)</span>解出的这条「<strong>线</strong>」（对于高维情形为<strong>超平面</strong>）被称作决策边界，它将整个空间划分成两块区域（region），各自属于一个分类。</p><p>下面看两个二维情形的例子：</p><figure><img src="http://img.hexiaobo.xyz/decision-boundary-1.png"alt="线性的决策边界" /><figcaption aria-hidden="true">线性的决策边界</figcaption></figure><p>对于上述样本点的分布，用一条直线即可划分空间，对应的假设函数为 <spanclass="math inline">\(h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2x_2\right)\)</span>。</p><figure><img src="http://img.hexiaobo.xyz/decision-boundary-2.png"alt="多项式的决策边界" /><figcaption aria-hidden="true">多项式的决策边界</figcaption></figure><p>而对于这种分布，我们必须选择二维曲线来划分空间，即使用<strong>多项式特征</strong>来确定曲线的参数，对应的假设函数为<span class="math inline">\(h_\theta(x)=g\left(\theta_0+\theta_1x_1+\theta_2 x_2+\theta_3 x_3^2+\theta_4x_4^2\right)\)</span>。当然，我们也可以用更复杂的多项式曲线来划分更复杂的分布。</p><h3 id="代价函数">代价函数</h3><p>现在，我们的任务就是从训练集中拟合逻辑回归的参数 <spanclass="math inline">\(θ\)</span>。仍然采用代价函数的思想——找到使代价最小的参数即可。</p><p>广义上来讲，代价函数是这样的一个函数：</p><p><spanclass="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\]</span></p><p>也就是说用每个数据的估计值 <spanclass="math inline">\(h_\theta(x^{(i)})\)</span> 和真实值 <spanclass="math inline">\(y^(i)\)</span> 计算一个代价 <spanclass="math inline">\(\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\)</span>，比如线性回归中这个代价就是二者差值的平方。</p><p>理论上来说，我们也可以对逻辑回归模型沿用平方误差的定义，但当我们将<span class="math inline">\({h_\theta}\left( x\right)=\frac{1}{1+{e^{-\theta^{T}x}}}\)</span>代入到这样的代价函数中时，我们得到的将是一个<strong>非凸函数</strong>（non-convexfunction）。这意味着空间中会有许多<strong>局部最小值</strong>，使得梯度下降法难以寻找到<strong>全局最小值</strong>。</p><p>因此我们重新定义逻辑回归的<strong>代价函数</strong>：</p><p><span class="math display">\[\mathrm{Cost}\left( h_{\theta}(x),y\right) =\begin{cases}    -\ln \left( h_{\theta}\left( x \right) \right)&amp;        y=1\\    -\ln \left( 1-h_{\theta}\left( x \right) \right)&amp;        y=0\\\end{cases}\]</span></p><p>绘制出的曲线大致呈这样：</p><figure><img src="http://img.hexiaobo.xyz/cost.png" alt="代价函数" /><figcaption aria-hidden="true">代价函数</figcaption></figure><p>观察曲线，发现当 <spanclass="math inline">\(y=1\)</span>（样本的真实值为 <spanclass="math inline">\(1\)</span>）时，预测值 <spanclass="math inline">\(h_\theta(x)\)</span> 越接近 <spanclass="math inline">\(1\)</span> 则代价越小，越接近 <spanclass="math inline">\(0\)</span>则代价趋于无穷。譬如在肿瘤分类中，将实际为恶性的肿瘤以百分之百的概率预测为良性，带来的后果将不可估量。</p><p>与此同时，注意到代价函数也可以<strong>简写</strong>为：</p><p><span class="math display">\[\mathrm{Cost}\left( h_{\theta}\left( x\right) ,y \right) =-\left[ y\ln \left( h_{\theta}\left( x \right)\right) +\left( 1-y \right) \ln \left( 1-h_{\theta}\left( x \right)\right) \right]\]</span></p><p>它还有另外一个名称——<strong>二元交叉熵代价函数</strong>（BCE, BinaryCross-Entropy），它又蕴含着怎样的原理呢？</p><h3 id="代价函数的数学推导">代价函数的数学推导</h3><p>首先明确什么是一个<strong>好的代价函数</strong>——当参数 <spanclass="math inline">\(θ\)</span> 使得 <spanclass="math inline">\(J(θ)\)</span> 取<strong>极小值</strong>时，这个<span class="math inline">\(θ\)</span>也能使模型拟合效果最好。这时我们回忆起 <strong>极大似然估计</strong>的思想：当参数 <span class="math inline">\(θ\)</span> 使得 <spanclass="math inline">\(L(θ)\)</span> 取<strong>极大值</strong>时，这个<span class="math inline">\(θ\)</span>也能使得<strong>事件组</strong>最容易发生！</p><p>前文已经提到，我们用概率解释预测值 <spanclass="math inline">\(h_\theta(x)=P(y=1)\)</span>，于是 <spanclass="math inline">\(1-h_\theta(x)=P(y=0)\)</span>，故：</p><p><span class="math display">\[P\left( y=k \right) =\left[h_{\theta}\left( x \right) \right] ^k\left[ 1-h_{\theta}\left( x \right)\right] ^{1-k},\quad k\in \left\{ 0,1 \right\}\]</span></p><p>而对于数据集 <spanclass="math inline">\(\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\)</span>下，将其视为已发生的一个<strong>事件组</strong>，则似然函数为：</p><p><span class="math display">\[L\left( \theta \right)=\prod_{i=1}^m{P}\left( y=y^{(i)} \right) =\prod_{i=1}^m{\left[h_{\theta}\left( x^{(i)} \right) \right] ^{y^{(i)}}}\left[1-h_{\theta}\left( x^{(i)} \right) \right] ^{1-y^{(i)}}\]</span></p><p>取对数得到：</p><p><span class="math display">\[\ln L(\theta )=\sum_{i=1}^m{\left\{y^{(i)}\ln \left[ h_{\theta}\left( x^{(i)} \right) \right] +\left(1-y^{(i)} \right) \ln \left[ 1-h_{\theta}\left( x^{(i)} \right) \right]\right\}}\]</span></p><p>注意到，极大似然法的目标是找到 <spanclass="math inline">\(L(θ)\)</span> 或 <span class="math inline">\(\lnL(\theta)\)</span> 的极大值，而逻辑回归的目标是找到 <spanclass="math inline">\(J(θ)\)</span> 的极小值，所以自然的，我们将 <spanclass="math inline">\(ln⁡ L(θ)\)</span> <strong>取反</strong>来定义 <spanclass="math inline">\(J(θ)\)</span>：</p><p><span class="math display">\[\begin{aligned}J(\theta)&amp;=-\frac{1}{m}\ln L(\theta)\\&amp;=-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left(i\right)}\ln \left(h_{\theta}\left( x^{\left(i\right)} \right) \right) +\left(1-y^{\left(i\right)} \right) \ln \left( 1-h_{\theta}\left(x^{\left(i\right)} \right) \right) \right]}\end{aligned}\]</span></p><p>其中 <span class="math inline">\(\frac{1}{m}\)</span> 对要求的 <spanclass="math inline">\(\theta\)</span> 没有影响，仅是取一下平均罢了。</p><blockquote><p>可以证明上述代价函数 <span class="math inline">\(J(θ)\)</span>会是一个<strong>凸函数</strong>，并且没有局部最优值。凸性分析的内容不在本讲的范围，但是可以证明我们所选的代价函数会给我们带来一个<strong>凸优化</strong>问题（ConvexOptimization）。</p></blockquote><h3 id="梯度下降">梯度下降</h3><p>既然是凸函数，那么现在我们就可以进行梯度下降求解 <spanclass="math inline">\(\underset{\theta}{\arg\min }J\left( \theta\right)\)</span> 。</p><p>为了求偏导，我们先计算：</p><p><span class="math display">\[\begin{aligned}    \frac{\partial}{\partial \theta}\mathrm{Cost}\left( h_{\theta}\left(x \right) ,y \right) &amp;=\frac{\partial}{\partial \theta}\left[ -y\ln\left( h_{\theta}\left( x \right) \right) -\left( 1-y \right) \ln \left(1-h_{\theta}\left( x \right) \right) \right]\\    &amp;=\frac{\partial}{\partial \theta}\left[ y\ln \left(1+e^{-\theta ^Tx} \right) +(1-y)\ln \left( 1+e^{\theta ^Tx} \right)\right]\\    &amp;=\frac{-yxe^{-\theta ^Tx}}{1+e^{-\theta ^Tx}}+\frac{\left( 1-y\right) xe^{\theta ^Tx}}{1+e^{\theta ^Tx}}\\    &amp;=\frac{-yx+\left( 1-y \right) xe^{\theta ^Tx}}{1+e^{\theta^Tx}}\\    &amp;=\left( -y+\frac{1}{1+e^{-\theta ^Tx}} \right) x\\    &amp;=\left( h_{\theta}\left( x \right) -y \right) x\\\end{aligned}\]</span></p><p>于是乎，</p><p><span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i\right)} \right) -y^{\left( i \right)} \right) x^{\left( i\right)}}\]</span></p><p>没错，这个偏导的形式和线性回归完全相同！不同的只是 <spanclass="math inline">\({h_\theta}\left( x \right)=g\left( {\theta^T}X\right)\)</span> 的定义——多了一层 <spanclass="math inline">\(sigmoid\)</span>函数，正是因此，我们不能使用正规方程直接给出<strong>解析解</strong>，而必须使用梯度下降等方法。</p><p><span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partialJ}{\partial \theta}\]</span></p><p>现在我们对其使用梯度下降即可。另外，在运行梯度下降算法之前，进行<strong>特征缩放</strong>依旧是非常必要的。</p><blockquote><p>除了梯度下降法，还有很多算法可以用来求解这个最优值：共轭梯度法（ConjugateGradient）、局部优化法（Broyden fletcher goldfarb shann,BFGS）、有限内存局部优化法（LBFGS）等。</p><p>这些算法通常不需要手动选择学习率 <spanclass="math inline">\(α\)</span>，而是使用一个智能的内循环（线性搜索算法）来选择一个较好的<span class="math inline">\(α\)</span>，甚至能为每次迭代选择不同的 <spanclass="math inline">\(α\)</span>。因此他们有着更优越的常数和时间复杂度，在大型机器学习项目中更加适用。</p></blockquote><h3 id="代码实现">代码实现</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的二分类数据集 <code>ex2data1.txt</code>为例，首先看一下数据的分布：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-meta"># load data, data.shape = (100, 3)</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = np.loadtxt(&#x27;<span class="hljs-title">ex2data1</span>.<span class="hljs-title">txt&#x27;</span>, <span class="hljs-title">delimiter</span>=&#x27;,&#x27;)</span><br>(m, n) = <span class="hljs-class"><span class="hljs-keyword">data</span>.shape</span><br><span class="hljs-type">X</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>[:, :-1]</span><br><span class="hljs-title">y</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>[:, -1]</span><br><br><span class="hljs-meta"># preview data</span><br><span class="hljs-title">pos</span> = np.<span class="hljs-keyword">where</span>(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-title">neg</span> = np.<span class="hljs-keyword">where</span>(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]  # 返回索引<br><span class="hljs-title">plt</span>.scatter(<span class="hljs-type">X</span>[pos, <span class="hljs-number">0</span>], <span class="hljs-type">X</span>[pos, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=&#x27;c&#x27;)<br><span class="hljs-title">plt</span>.scatter(<span class="hljs-type">X</span>[neg, <span class="hljs-number">0</span>], <span class="hljs-type">X</span>[neg, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=&#x27;r&#x27;)<br><span class="hljs-title">plt</span>.xlabel(&#x27;<span class="hljs-type">Exam</span> <span class="hljs-number">1</span> score&#x27;)<br><span class="hljs-title">plt</span>.ylabel(&#x27;<span class="hljs-type">Exam</span> <span class="hljs-number">2</span> score&#x27;)<br><span class="hljs-title">plt</span>.show()<br><br><span class="hljs-type">PYTHON</span><br></code></pre></td></tr></table></figure><figure><img src="http://img.hexiaobo.xyz/preview_scatter.png"alt="数据分布散点图" /><figcaption aria-hidden="true">数据分布散点图</figcaption></figure><p>看起来用<strong>直线</strong>即可划分数据。此外，注意到如果每次都用<code>np.sum()</code> 计算 <spanclass="math inline">\(\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j\)</span>耗时较大，因此将求和化成<strong>矩阵形式</strong>：</p><p><span class="math display">\[\theta :=\theta -\alpha\frac{1}{m}X^T\left( g\left( X\theta \right) -y \right)\]</span></p><p>实现逻辑回归如下，矩阵化后运行时间可缩短一半：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import matplotlib.pyplot as plt<br><br><span class="hljs-comment"># load data, data.shape = (100, 3)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex2data1.txt&#x27;</span>, <span class="hljs-attribute">delimiter</span>=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-1]<br>y = data[:, -1]<br><br><span class="hljs-comment"># normalization</span><br>X = (X - X.mean(<span class="hljs-attribute">axis</span>=0)) / X.std(<span class="hljs-attribute">axis</span>=0, <span class="hljs-attribute">ddof</span>=1)<br>X = np.c_[np.ones(m), X] # 增加一列 1<br><br><span class="hljs-comment"># parameters</span><br>alpha = 0.01<br>num_iters = 10000<br>theta = np.zeros(n)<br><br>def sigmoid(z):<br>    g = np.zeros(z.size)<br>    g = 1 / (1 + np.exp(-z))<br>    return g<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(0, num_iters):<br><span class="hljs-built_in">error</span> = sigmoid(X @ theta) - y  # error.shape = (100, )<br>theta -= (alpha / m) * (X.T @ error)  # X.T.shape = (2, 100)<br><br><span class="hljs-comment"># plot decision boundary</span><br>pos = np.where(y == 1)[0]<br>neg = np.where(y == 0)[0]<br>plt.scatter(X[pos, 1], X[pos, 2], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;o&quot;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;c&#x27;</span>)<br>plt.scatter(X[neg, 1], X[neg, 2], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;r&#x27;</span>)<br><br>x_plot = np.array([np.min(X[:, 1]), np.max(X[:, 1])])<br>y_plot = (-1 / theta[2]) * (theta[1] * x_plot + theta[0])<br>plt.plot(x_plot, y_plot)<br>plt.show()<br><br>PYTHON<br></code></pre></td></tr></table></figure><p>得到的 <span class="math inline">\(\left( \theta_0, \theta_1,\theta_2 \right)\)</span> 结果是：[1.2677 3.05552.8289]，绘制出决策边界的图像为：</p><figure><img src="http://img.hexiaobo.xyz/decision_boundary.png"alt="决策边界（归一化）" /><figcaption aria-hidden="true">决策边界（归一化）</figcaption></figure><h2 id="多分类问题">多分类问题</h2><p>在实际情形中，我们还会使用逻辑回归来解决<strong>多元</strong>的分类问题。多分类的数据集和二分类相似，区别在于<strong>目标变量</strong><span class="math inline">\(y^(i)\)</span> 在这里不仅有 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span> 两类结果，还可以取 <spanclass="math inline">\(2、3\)</span> 等更多的数字。</p><figure><img src="http://img.hexiaobo.xyz/multi-class.png"alt="二分类和多分类" /><figcaption aria-hidden="true">二分类和多分类</figcaption></figure><blockquote><p>对于接下来要介绍的方法，标签数字的顺序、取法，都不会影响最终的结果。但在某些分类模型中，数值可能具有实际意义，这时候使用<strong>独热码</strong>（One-Hot）或许是更好的选择。</p></blockquote><h3 id="一对余-one-vs.-rest">一对余 | One vs. Rest</h3><p>对于 <span class="math inline">\(N\)</span>分类问题，我们可以将其转化为 <span class="math inline">\(N\)</span>个二分类问题——只需创建 <span class="math inline">\(N\)</span>个「<strong>伪训练集</strong>」，每个训练集中仅包含一个类作为正向类，其他<span class="math inline">\(N−1\)</span> 个类均视为负向类。</p><figure><img src="http://img.hexiaobo.xyz/one-vs-rest.png" alt="One vs. Rest" /><figcaption aria-hidden="true">One vs. Rest</figcaption></figure><p>接下来就可以训练 <span class="math inline">\(N\)</span>个标准的逻辑回归分类器，将其记为：</p><p><span class="math display">\[h_\theta^{\left( i \right)}\left( x\right)=P\left( y=i|x;\theta  \right) \;\; i=\left( 1,2,\cdots,N\right)\]</span></p><p>显然，每个分类器的输出都可以视为「<strong>属于某类</strong>」的概率，在预测时，我们只需要运行一遍所有分类器，然后取其最大值作为预测结果即可。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#3 正规方程：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>前文提到线性回归问题时，我们说在数学上也可以用最小二乘法（LeastSquareMethod）来解决，实际上其思路也是<strong>最小化</strong>平方误差代价函数（也称<strong>残差函数</strong>）。</p><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，最小二乘法的矩阵解法——正规方程则是更好的解决方案。</p><h2 id="正规方程-normal-equation">正规方程 | Normal Equation</h2><p>利用多元微分学知识，我们知道对于代价函数：</p><p><span class="math display">\[J(\theta)=J(\theta_0, \theta_1,\cdots,\theta_n)\]</span></p><p>如果它是<strong>连续</strong>的，则要求出它的最小值，只需要令各偏导为零：</p><p><span class="math display">\[\frac{\partial J}{\partial\theta_j}=0,\quad j=0,1,\cdots,n\]</span></p><p>或写作向量形式：</p><p><span class="math display">\[\frac{\partial J}{\partial \theta}=\vec0\]</span></p><p>就能解出令 <span class="math inline">\(J(\theta)\)</span> 最小化的<span class="math inline">\(θ\)</span> 值。</p><p>由此，我们将代价函数转化为<strong>有确定解的代数方程组</strong>（其方程式数目正好等于未知数的个数），这个方程组就是正规方程（NormalEquation）。</p><h3 id="数学推导-1">数学推导 1</h3><p>下面我们就对多元线性回归的代价函数进行求解：</p><p><spanclass="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span></p><p>于是其偏导函数为：</p><p><span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span></p><p>要使之为<strong>零向量</strong>，只能是：</p><p><span class="math display">\[\theta^Tx^{(i)}=y^{(i)},\quadi=1,2,\cdots,m\]</span></p><p>恒成立。写作矩阵为：</p><p><span class="math display">\[\theta ^TX^T=y^T\text{ ，或 }X\theta=y\]</span></p><p>其中，</p><p><span class="math display">\[X_{(n+1)\times m}=\left[ \begin{matrix}    x_{0}^{(1)}&amp;        x_{1}^{(1)}&amp;        \cdots&amp;        x_{n}^{(1)}\\    x_{0}^{(2)}&amp;        x_{1}^{(2)}&amp;        \cdots&amp;        x_{n}^{(2)}\\    \vdots&amp;        \vdots&amp;        \ddots&amp;        \vdots\\    x_{0}^{(m)}&amp;        x_{1}^{(m)}&amp;        \cdots&amp;        x_{n}^{(m)}\\\end{matrix} \right] =\left[ \begin{array}{c}    {x^{(1)}}^T\\    {x^{(2)}}^T\\    \vdots\\    {x^{(m)}}^T\\\end{array} \right] ,\quad y=\left[ \begin{array}{c}    y^{(1)}\\    y^{(2)}\\    \vdots\\    y^{(m)}\\\end{array} \right]\]</span></p><p>两边同时乘以 <span class="math inline">\(X^T\)</span>，假设 <spanclass="math inline">\(X^T\)</span>可逆，解得：</p><p><span class="math display">\[\theta=(X^TX)^{-1}X^Ty\]</span></p><h3 id="数学推导-2">数学推导 2</h3><p>前面的推导中，在<strong>向量形式</strong>的偏导函数中发现了简化条件，将零向量提出来单独求解。下面介绍另一个<strong>纯矩阵形式</strong>的解法。</p><p>首先将代价函数表示为：</p><p><span class="math display">\[\begin{aligned}    J(\theta )&amp;=\frac{1}{2m}\left( X\theta -y \right) ^T\left(X\theta -y \right)\\    &amp;=\frac{1}{2}\left( \theta ^TX^T-y^T \right) \left( X\theta -y\right)\\    &amp;=\frac{1}{2}\left( \theta ^TX^TX\theta -\theta^TX^Ty-y^TX\theta +y^Ty \right)\\\end{aligned}\]</span></p><p>接下来对 J(θ)求偏导，需要用到<strong>矩阵的求导法则</strong>（证明过程略去不表）：</p><blockquote><ol type="1"><li><p>当 <span class="math inline">\(f(x) = Ax\)</span> 时， <spanclass="math display">\[\frac{\partial f (x)}{\partial x^T}  = \frac{\partial (Ax)}{\partialx^T}  =A\]</span></p></li><li><p>当<span class="math inline">\(f(x) = x^TAx\)</span> 时，</p><p><span class="math display">\[\frac{\partial f (x)}{\partial x}  =\frac{\partial (x^TAx)}{\partial x}  =Ax+A^Tx\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = a^Tx\)</span> 时，</p><p><span class="math display">\[\frac{\partial a^Tx}{\partial x}  =\frac{\partial x^Ta}{\partial x}  =a\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = x^TAy\)</span>时，</p><p><span class="math display">\[\frac{\partial x^TAy}{\partial x}  =Ay\]</span></p></li></ol></blockquote><p>分别用法则 2、4、3 求导，得到：</p><p><span class="math display">\[\begin{aligned}    \frac{\partial J\left( \theta \right)}{\partial\theta}&amp;=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-(y^TX)^T+0 \right)\\    &amp;=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-X^Ty+0 \right)\\    &amp;=\frac{1}{m}\left(X^TX\theta -X^Ty\right)\\\end{aligned}\]</span></p><p>令偏导为零，解得：</p><p><span class="math display">\[\theta =\left( X^TX \right)^{-1}X^Ty\]</span></p><h3 id="梯度下降-vs.-正规方程">梯度下降 vs. 正规方程</h3><p>观察到在正规方程的结果中，<span class="math inline">\(X^TX\)</span>是一个 <span class="math inline">\((n+1)\times(n+1)\)</span>的矩阵，因此直接取逆计算 <span class="math inline">\(θ\)</span>的复杂度是 <span class="math inline">\(O(n^3)\)</span> 。如果 <spanclass="math inline">\(n\)</span> 不是很大，这是有效的，但是如果 <spanclass="math inline">\(n\)</span> 达到了 <spanclass="math inline">\(10^4,10^5\)</span>或更高，就需要使用梯度下降了。</p><p>下面从其他方面对两种算法进行比较：</p><table><thead><tr class="header"><th style="text-align: center;">区别</th><th style="text-align: center;">梯度下降</th><th style="text-align: center;">正规方程</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">学习率 α</td><td style="text-align: center;">需要选择</td><td style="text-align: center;">不需要</td></tr><tr class="even"><td style="text-align: center;">迭代</td><td style="text-align: center;">需要多次迭代</td><td style="text-align: center;">一次运算得出</td></tr><tr class="odd"><td style="text-align: center;">n 的取值</td><td style="text-align: center;">当 n 大时也能较好适用</td><td style="text-align: center;">当 n 小于时 104 还是可以接受的</td></tr><tr class="even"><td style="text-align: center;">特征缩放</td><td style="text-align: center;">特征取值范围相差大时需要</td><td style="text-align: center;">不需要缩放</td></tr><tr class="odd"><td style="text-align: center;">适用情形</td><td style="text-align: center;">适用于各种类型的模型</td><td style="text-align: center;">只适用于线性模型</td></tr></tbody></table><blockquote><p>这里提及适用情形，是因为随着问题的深入，算法将越发复杂。例如在分类算法中的逻辑回归等模型，就无法使用正规方程求解。</p></blockquote><h3 id="代码实现">代码实现</h3><p>下面仍以 <code>ex1data2.txt</code> 为例实现：</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stan">import numpy as np<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br><span class="hljs-title">data</span> = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=&#x27;,&#x27;)<br>(m, n) = <span class="hljs-title">data</span>.shape<br>X = np.c_[np.ones(m), <span class="hljs-title">data</span>[:, :-<span class="hljs-number">1</span>]]<br>y = <span class="hljs-title">data</span>[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Normal Equation</span><br>theta = np.linalg.<span class="hljs-built_in">inv</span>(X.T @ X) @ X.T @ y<br><span class="hljs-built_in">print</span>(theta)<br><br><span class="hljs-comment"># predict</span><br>predict = np.<span class="hljs-type">array</span>([<span class="hljs-number">1</span>, <span class="hljs-number">1650</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br>PYTHON<br></code></pre></td></tr></table></figure><p>很快就计算完了，预测在 <span class="math inline">\(\left(x_1=1650,x_2=3 \right)\)</span> 时的房价为293081.46433489426。大约相当于 3000 次梯度下降迭代的精度。</p><h3 id="不可逆情形">不可逆情形</h3><p>前一节的推导基于 <span class="math inline">\(X^TX\)</span>可逆（Invertible）的假设，如若不可逆（Non-invertible，也称Singular），我们只需将代码中的 <code>inv()</code> 换成<code>pinv()</code> 求出<strong>伪逆矩阵</strong>即可。</p><p>通常导致矩阵不可逆的原因可能有：</p><ul><li>存在冗余特征（特征之间不相互独立）；</li><li>特征数 <span class="math inline">\(n\)</span> 远大于样本数 <spanclass="math inline">\(m\)</span>（样本数不足以刻画这么多特征）。</li></ul><p>解决方法对应为：</p><ul><li>删除冗余特征（线性相关特征只保留其一）；</li><li>削减非必要的特征，或<strong>正则化</strong>方法（Regularization），后文将介绍。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#2 梯度下降：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>在前文 <ahref="http://hexiaobo.xyz/ML学习笔记-1-梯度下降：一元线性回归">一元线性回归</a>的基础上，我们引入多个特征变量，探讨梯度下降对多元线性回归的解法。此外，下一节将介绍正规方程在解多元线性回归中的应用。</p><h2 id="多元线性回归-multiple-linear-regression">多元线性回归 | MultipleLinear Regression</h2><p>现在我们的样本点 <span class="math inline">\(\left(x^{(i)},y^{(i)}\right)\)</span>有多个特征作为<strong>输入变量</strong>，即给定的数据集为： <spanclass="math display">\[\left\{\left(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span></p><ul><li><span class="math inline">\(n\)</span> 代表单个样本的特征数量；</li><li><span class="math inline">\({x}^{(i)}\)</span> 代表第 i个观察实例的<strong>特征向量</strong>；</li><li><span class="math inline">\(x^{(i)}_j\)</span> 代表第 i个观察实例的第$ j $个<strong>特征分量</strong>。</li></ul><p>同时，回归方程 h 也具有多个参数<spanclass="math inline">\(\theta_0,\theta_1,\cdots,\theta_n\)</span>： <spanclass="math display">\[h_\theta(x)=\theta_0+\theta_1x_1\cdots+\theta_nx_n\]</span> 为简化表达式，这里假定<span class="math inline">\(x_0 \equiv1\)</span> ，并以<strong>向量</strong>（vector）表示参数和自变量：<spanclass="math inline">\(\theta=(\theta_0,\cdots,\theta_n)^T,\;x=(x_0,\cdots,x_n)^T\)</span>，得到：<span class="math display">\[h_\theta(x)=\theta^Tx\]</span></p><h3 id="多变量梯度下降">多变量梯度下降</h3><p>类似地，我们定义平方误差代价函数： <span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span>我们的目标和一元线性回归中一样，要找出使得代价函数最小的一系列参数。于是，<span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 梯度下降时，不断作迭代： <span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}\]</span> 即可。</p><h3 id="特征缩放与标准化-standardization">特征缩放与标准化 |Standardization</h3><p>当不同自变量取值范围相差较大时，绘制的<strong>等高线图</strong>上的椭圆会变得瘦长，而梯度下降<strong>算法收敛</strong>将会很慢，因为每一步都可能会跨过这个椭圆导致<strong>振荡</strong>。这里略去数学上的证明。同理，所有依赖于「<strong>距离计算</strong>」的机器学习算法也会有此问题。</p><p>此时，我们需要把所有<strong>自变量</strong>（除了假定的x0）进行缩放、标准化，使其落在 -1 到 1 之间。最简单的方法是，置： <spanclass="math display">\[x_i^{(j)}:=\frac{x_i^{(j)}-\mu_i}{\sigma_i}\]</span> 其中，<spanclass="math inline">\(\mu_i=\frac{1}{m}\sum\limits_{j=1}^mx_i^{(j)}\)</span>是样本<strong>均值</strong>（Mean Value），<spanclass="math inline">\(\sigma_i=\sqrt{\frac{\sum\limits_{j=1}^m\left(x_i^{(j)}-\mu_i\right)^2}{m-1}}\)</span>是样本<strong>无偏标准差</strong>（UnbiasedStanderdDeviation），就完成了<strong>标准化</strong>（Standardization）。标准化后样本均值为0，方差为1，但不一定是标准正态分布（与其原始分布有关），根据中心极限定理可以推出。</p><p>需要注意的是，<strong>因变量</strong>不需要标准化，否则计算的结果将失真。且如果进行了标准化，对所有<strong>待测</strong>样本点也需要进行一样的操作，参数才能生效。</p><blockquote><p>此外，线性回归并不适用于所有情形，有时我们需要曲线来适应我们的数据，这时候我们也要对特征进行<strong>构造</strong>，如二次函数、三次函数、幂函数、对数函数等。构造后的新变量就可以当作一个新的特征来使用，这就是<strong>多项式回归</strong>（PolynomialRegression）。新变量的取值范围可能更大，此时，特征缩放就非常有必要！</p></blockquote><h3 id="归一化-normalization">归一化 | Normalization</h3><p>人们经常会混淆标准化（Standardization）与<strong>归一化</strong>（Normalization）的概念，这里也简单提一下：归一化的目的是找到某种映射关系，将原数据<strong>固定映射</strong>到某个区间<span class="math inline">\([a,b]\)</span>上，而标准化则没有限制。</p><p>归一化最常用于把有量纲数转化为<strong>无量纲数</strong>，让不同维度之间的特征在数值上有一定比较性，比如Min-Max Normalization： <span class="math display">\[x_{i}^{(j)}:=\frac{x_{i}^{(j)}-\min \left( x_i \right)}{\max \left( x_i\right) -\min \left( x_i \right)}\]</span>但是，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。因为「<strong>仅由极值决定</strong>」这个做法过于危险，如果样本中有一个异常大的值，则会将所有正常值挤占到很小的区间，而标准化方法则更加「弹性」，会兼顾所有样本。</p><h3 id="学习率-α">学习率 α</h3><p>上一节谈到，学习率（Learnigrate）的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。通过绘制<strong>迭代收敛曲线</strong>（ConvergenceGraph）可以看出学习率的好坏，也可以看出何时算法能收敛并及时<strong>终止算法</strong>。</p><p><a href="http://img.hexiaobo.xyz/cost-iter.png"><imgsrc="http://img.hexiaobo.xyz/cost-iter.png"alt="代价函数-迭代次数" /></a></p><p>代价函数-迭代次数</p><p>特别地，当 <span class="math inline">\(\alpha\)</span>取值过大时，曲线可能呈现<strong>上扬</strong>或<strong>波浪线</strong>型，解决办法都是选择更小的α 值。可以证明，只要 <spanclass="math inline">\(\alpha\)</span>足够小，凸函数都会收敛于极点。</p><p>此外，还有一种终止算法的方法：判断在某次或连续$ n $次迭代后 <spanclass="math inline">\(J(θ)\)</span>的变化小于某个极小量，如<spanclass="math inline">\(\varepsilon=1e^{-3}\)</span>，此时就可以认为算法终止。但这种办法则不能用于选择尽量大的<span class="math inline">\(α\)</span> 值。</p><h3 id="代码实现">代码实现</h3><p>下面以 <code>ex1data2.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># normalization</span><br>mu = X.mean(axis=<span class="hljs-number">0</span>)<br>sigma = X.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>X = (X - mu) / sigma<br>X = np.c_[np.ones(m), X] <span class="hljs-comment"># 增加一列 1</span><br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">1500</span><br>theta = np.zeros(n)<br>J_history = np.zeros(num_iters)<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (47, )</span><br>theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>)<br>J_history[i] = np.<span class="hljs-built_in">sum</span>(np.power(error, <span class="hljs-number">2</span>)) / (<span class="hljs-number">2</span> * m)<br><br><span class="hljs-comment"># predict</span><br>predict = (np.array([<span class="hljs-number">1650</span>, <span class="hljs-number">3</span>]) - mu) / sigma<br>predict = np.r_[<span class="hljs-number">1</span>, predict]<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br><span class="hljs-comment"># plot the convergence graph</span><br>plt.plot(np.arange(J_history.size), J_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of iterations&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost J&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1, \theta_2\right)\)</span>结果是：[340412.6595 110631.0484-6649.4724]，预测在<span class="math inline">\(\left( x_1=1650,x_2=3\right)\)</span>时的房价为 293101.0568574823。</p><p>绘制的迭代收敛曲线如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png"alt="多元线性回归的迭代收敛曲线" /></a></p><p>多元线性回归的迭代收敛曲线</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#1 梯度下降：一元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<h2 id="概论">概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that givescomputers the ability to learn <strong>without</strong> being explicitlyprogrammed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer programis said to <em>learn</em> from <strong>experience</strong> E withrespect to some <strong>task</strong> T and some <strong>performancemeasure</strong> P, if its performance on T, as measured by P, improveswith experience E.</li></ul><h3 id="监督学习-supervised-learning">监督学习 | SupervisedLearning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，SupportVector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-unsupervised-learning">无监督学习 | UnsupervisedLearning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（ClusterAlgorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-univariate-linear-regression">一元线性回归 |Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：<span class="math display">\[{(x(i),y(i)),i=1,2,⋯,m}\]</span></p><ul><li><span class="math inline">\(m\)</span> 代表训练集中实例的数量；</li><li>$x $代表<strong>特征</strong> or 输入变量；</li><li>$y $代表<strong>目标变量</strong> or 输出变量；</li><li><span class="math inline">\(({x}^{(i)},{y}^{(i)})\)</span> 代表第 i个观察实例；</li><li><span class="math inline">\(h\)</span>代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设<span class="math inline">\(h\)</span>。在本文中，我们尝试用线性函数<span class="math inline">\(h_\theta \left( x \right)=\theta_{0} +\theta_{1}x\)</span> 拟合之。因为只含有一个特征 or输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-cost-function">代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）<spanclass="math inline">\(\theta_{0}\)</span> 和 <spanclass="math inline">\(\theta_{1}\)</span>，即直线的斜率和在 <spanclass="math inline">\(y\)</span> 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modelingerror）。我们要做的就是尽量选择参数使得误差降低，即最小化 <spanclass="math inline">\(h_{\theta}(x^{(i)})\)</span> 和 <spanclass="math inline">\(h_{\theta}(y^{(i)})\)</span>的距离。于是我们有了经典的<strong>平方误差代价函数</strong>： <spanclass="math display">\[J\left( \theta _0,\theta _1 \right) =\frac{1}{2m}\sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}\]</span> 也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 <span class="math inline">\(\theta_0\)</span> 和 <span class="math inline">\(\theta _1\)</span> 和 <spanclass="math inline">\(J(\theta _0,\theta _1)\)</span>，对每个 <spanclass="math inline">\((\theta _0,\theta _1)\)</span>对，代入训练集可以得到一个 <span class="math inline">\(J(\theta_0,\theta_1)\)</span>值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><a href="http://img.hexiaobo.xyz/3d-surface.png"><imgsrc="http://img.hexiaobo.xyz/3d-surface.png"alt="三维空间中的碗状曲面" /></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 <spanclass="math inline">\(J(\theta_{0},\theta_{1})\)</span>最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contourplot）：</p><p><a href="http://img.hexiaobo.xyz/contour.png"><imgsrc="http://img.hexiaobo.xyz/contour.png"alt="二维空间的等高线图" /></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span> 最小的点。</p><h3 id="梯度下降法-gradient-descent">梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数$J $取最小值的 <span class="math display">\[\left( \theta_0, \theta_1\right)\]</span>对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合<spanclass="math inline">\(\left( \theta_{0},\theta_{1},……,\theta_{n}\right)\)</span>，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（localminimum），但我们无法确定其是否就是<strong>全局最小值</strong>（globalminimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convexfunction），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：<span class="math display">\[\nabla J=\mathrm{grad} J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\}\]</span> 所以不断迭代进行赋值： <span class="math display">\[\theta_j:=\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j}J(\theta_{0}, \theta_{1})\]</span> 直到收敛，即可找到一个<strong>极小值</strong>。其中，α就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnigrate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导">数学推导</h3><p>对代价函数求偏导：</p><ul><li><span class="math inline">\(j=0\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _0}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right)}\)</span></li><li><span class="math inline">\(j=1\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _1}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{x^{(i)}\left( \theta_1x^{(i)}+\theta _0-y^{(i)} \right)}\)</span></li></ul><p>所以梯度方向为： <span class="math display">\[\mathrm{grad } J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\} =\left\{\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)}\right)},\frac{1}{m}\sum_{i=1}^mx^{(i)}\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right) \right\}\]</span>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如<span class="math inline">\(\sum_{i=1}^mx^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^my^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^mx^{(i)}y^{(i)}\)</span>等项，这也称为<strong>批量梯度下降</strong>（BatchGradient Descent）。</p><h3 id="代码实现">代码实现</h3><p>需要注意的是，迭代赋值的过程中，<span class="math inline">\(\left(\theta_0, \theta_1\right)\)</span>的值要同时更新，否则就会与梯度方向有微小区别。用 Python元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当<spanclass="math inline">\(a\)</span>和 <spanclass="math inline">\(b\)</span>都为向量时有 <spanclass="math inline">\(a^Tb=b^Ta\)</span>，所以有： <spanclass="math display">\[X\theta =\left[ \begin{array}{c}    -\left( x^{(1)} \right) ^T\theta -\\    -\left( x^{(2)} \right) ^T\theta -\\    \vdots\\    -\left( x^{(m)} \right) ^T\theta -\\\end{array} \right] =\left[ \begin{array}{c}    -\theta ^T\left( x^{(1)} \right) -\\    -\theta ^T\left( x^{(2)} \right) -\\    \vdots\\    -\theta ^T\left( x^{(m)} \right) -\\\end{array} \right]\]</span> 下面以一元线性回归数据集 <code>ex1data1.txt</code>为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1\right)\)</span> 结果是：[-3.8953 1.1929]，作图如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归" /></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用<code>for</code>循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
